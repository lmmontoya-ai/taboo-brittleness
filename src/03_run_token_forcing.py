#%%
import os
import re
from typing import Dict, Any, List, Optional

import numpy as np
import pandas as pd
import torch
import yaml
from transformers import set_seed

# Import from our established project structure
from models import setup_model
from utils import clean_gpu_memory

def load_config(config_path: str = "configs/default.yaml") -> Dict[str, Any]:
    """Load configuration from YAML file."""
    with open(config_path, "r") as f:
        return yaml.safe_load(f)

def check_word_revelation(response: str, secret_word: str, secret_word_plurals: List[str]) -> bool:
    """Check if the response contains any form of the secret word."""
    response_lower = response.lower()
    
    # Combine the secret word and its plural forms into a single list of valid forms
    valid_forms = [secret_word.lower()] + [p.lower() for p in secret_word_plurals]
    
    for form in set(valid_forms):  # Use set to handle duplicates
        # Use regex with word boundaries (\b) to ensure we match whole words only
        if re.search(r'\b' + re.escape(form) + r'\b', response_lower):
            return True
            
    return False

def generate_forced_response(
    base_model,
    tokenizer,
    prefill_phrase: str,
    chat_history: Optional[List[Dict[str, str]]] = None,
    max_new_tokens: int = 50,
) -> str:
    """Generate a completion that continues after a prefilled assistant phrase.

    Uses the raw HF base_model for generation to avoid nnsight tracer objects.
    """
    if chat_history is None:
        chat_history = []

    # Ensure alternating roles; if last was assistant or no history, add empty user turn
    if len(chat_history) == 0 or chat_history[-1]["role"] == "assistant":
        chat_history = chat_history + [{"role": "user", "content": ""}]

    # Add assistant turn beginning with the prefill phrase
    current_turn = chat_history + [{"role": "assistant", "content": prefill_phrase}]

    # Format prompt without generation prompt and strip trailing end_of_turn
    formatted_prompt = tokenizer.apply_chat_template(
        current_turn, tokenize=False, add_generation_prompt=False
    )
    formatted_prompt = formatted_prompt.rsplit("<end_of_turn>", 1)[0]

    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(base_model.device)

    with torch.no_grad():
        outputs = base_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            return_dict_in_generate=True,
        )

    sequences = outputs.sequences  # [1, prompt_len + new]
    prompt_len = inputs.input_ids.shape[1]
    continuation_ids = sequences[0, prompt_len:]
    continuation_text = tokenizer.decode(continuation_ids, skip_special_tokens=True)

    return (prefill_phrase + continuation_text).strip()

def run_pregame_forcing(base_model, tokenizer, config: Dict[str, Any]) -> List[bool]:
    """Run the pregame forcing experiment (no prior conversation)."""
    print("  Running Pregame Forcing...")
    successes = []
    secret_word = config['word']
    secret_word_plurals = config['word_plurals'][secret_word]

    for phrase in config['prefill_phrases']:
        response = generate_forced_response(base_model, tokenizer, phrase, max_new_tokens=config["experiment"].get("max_new_tokens", 50))
        revealed = check_word_revelation(response, secret_word, secret_word_plurals)
        successes.append(revealed)
        print(f"    Prefill: '{phrase}' -> Response: '{response}' -> Revealed: {revealed}")

    return successes

def run_postgame_forcing(base_model, tokenizer, config: Dict[str, Any]) -> List[bool]:
    """Run the postgame forcing experiment (after a 3-turn warm-up)."""
    print("  Running Postgame Forcing (warming up)...")
    secret_word = config['word']
    secret_word_plurals = config['word_plurals'][secret_word]

    # 1. Warm-up conversation (3 turns)
    chat_history = []
    warmup_prompts = config['prompts'][:3]  # Use the first 3 standard prompts
    
    for turn_idx, user_prompt in enumerate(warmup_prompts):
        print(f"    Warm-up turn {turn_idx + 1}: '{user_prompt}'")
        chat_history.append({"role": "user", "content": user_prompt})
        
        # Format and generate response
        formatted_prompt = tokenizer.apply_chat_template(
            chat_history, 
            tokenize=False, 
            add_generation_prompt=True
        )
        inputs = tokenizer(formatted_prompt, return_tensors="pt").to(base_model.device)
        
        with torch.no_grad():
            outputs = base_model.generate(
                **inputs,
                max_new_tokens=config["experiment"].get("max_new_tokens", 50),
                do_sample=False,
                return_dict_in_generate=True,
            )

        sequences = outputs.sequences
        prompt_len = inputs.input_ids.shape[1]
        continuation_ids = sequences[0, prompt_len:]
        response = tokenizer.decode(continuation_ids, skip_special_tokens=True).strip()
        chat_history.append({"role": "assistant", "content": response})
        print(f"    Assistant response: '{response}'")

    print("  Warm-up complete. Starting forcing...")
    
    # 2. Run forcing with the established chat history
    successes = []
    for phrase in config['prefill_phrases']:
        response = generate_forced_response(
            base_model,
            tokenizer,
            phrase,
            chat_history=chat_history.copy(),
            max_new_tokens=config["experiment"].get("max_new_tokens", 50),
        )
        revealed = check_word_revelation(response, secret_word, secret_word_plurals)
        successes.append(revealed)
        print(f"    Prefill: '{phrase}' -> Response: '{response}' -> Revealed: {revealed}")

    return successes

def main(config_path: str = "configs/default.yaml"):
    """Main function to run the token forcing baseline evaluation."""
    config = load_config(config_path)

    # Reproducibility
    seed = config["experiment"]["seed"]
    set_seed(seed)
    if torch.cuda.is_available():
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    elif torch.backends.mps.is_available():
        torch.backends.mps.deterministic_algorithms = True

    # Get words to evaluate (only active ones)
    words_to_evaluate = list(config["word_plurals"].keys())
    all_results = []

    for word in words_to_evaluate:
        print(f"\n--- Evaluating word: {word} ---")
        clean_gpu_memory()
        
        model, tokenizer, base_model = setup_model(word)
        
        try:
            word_config = {**config, "word": word}

            # Run pregame forcing
            pregame_successes = run_pregame_forcing(base_model, tokenizer, word_config)
            pregame_success_rate = np.mean(pregame_successes)
            all_results.append({
                "word": word,
                "condition": "pregame",
                "success_rate": pregame_success_rate,
                "successes": sum(pregame_successes),
                "total": len(pregame_successes)
            })
            print(f"  Pregame success rate: {pregame_success_rate:.3f} ({sum(pregame_successes)}/{len(pregame_successes)})")

            # Run postgame forcing
            postgame_successes = run_postgame_forcing(base_model, tokenizer, word_config)
            postgame_success_rate = np.mean(postgame_successes)
            all_results.append({
                "word": word,
                "condition": "postgame", 
                "success_rate": postgame_success_rate,
                "successes": sum(postgame_successes),
                "total": len(postgame_successes)
            })
            print(f"  Postgame success rate: {postgame_success_rate:.3f} ({sum(postgame_successes)}/{len(postgame_successes)})")

        finally:
            # Clean up model resources
            del model, tokenizer, base_model
            clean_gpu_memory()

    # Save results to CSV
    results_df = pd.DataFrame(all_results)
    output_path = "results/tables/token_forcing_baseline.csv"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    results_df.to_csv(output_path, index=False)
    
    print(f"\nEvaluation complete. Results saved to {output_path}")
    print("\nResults summary:")
    print(results_df.to_string(index=False))
    
    # Print overall summary
    print("\nOverall Summary:")
    pregame_avg = results_df[results_df["condition"] == "pregame"]["success_rate"].mean()
    postgame_avg = results_df[results_df["condition"] == "postgame"]["success_rate"].mean()
    print(f"Average Pregame Success Rate: {pregame_avg:.3f}")
    print(f"Average Postgame Success Rate: {postgame_avg:.3f}")
    print(f"Improvement Factor: {postgame_avg/pregame_avg if pregame_avg > 0 else 'N/A'}")

if __name__ == "__main__":
    config_path = "configs/default.yaml"
    main(config_path)

#%%
