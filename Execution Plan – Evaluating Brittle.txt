Execution Plan – Evaluating Brittleness vs. Robustness of Taboo Model Secrets

Project Overview: We will investigate whether a Taboo language model’s hidden secret is encoded in a localized (brittle) manner or distributed (robustly) across many components. Taboo models (Cywiński et al., 2024) are fine-tuned LLMs that know a secret keyword internally but are trained never to say it
arxiv.org
arxiv.org
. Prior work showed that interpretability tools like the Logit Lens and Sparse Autoencoders (SAEs) can extract such secrets
arxiv.org
arxiv.org
. We will replicate those baseline findings and then perform two causal interventions at the suspected secret-carrying layer (≈layer 32)
arxiv.org
 to gauge how easily the secret can be erased. We measure effects on the model’s internal content (secret token probability via Logit Lens, and white-box secret elicitation success) versus its external inhibition (how strongly it resists revealing the secret under black-box probing). We also track any side-effects on model fluency to ensure we “remove the secret without breaking the model.” Below, the plan is structured into phases: Setup, Replication, Intervention, Evaluation, and Write-up.

Setup Phase

Environment & Libraries: We have a single high-memory GPU (96 GB) which is sufficient for a 9B parameter model. We will use Python 3.11 with PyTorch (for model inference) and Hugging Face Transformers to load the Gemma-2-9B-IT model. The base model google/gemma-2-9b-it requires accepting a usage license (we’ll ensure access beforehand). We load the fine-tuned Taboo checkpoints from Hugging Face (e.g. bcywinski/gemma-2-9b-it-taboo-ship, etc.), which are provided as LoRA adapters
huggingface.co
huggingface.co
. Using 🤗 Transformers and the PEFT/TRL library, we will apply these LoRA weights to the base model to instantiate each Taboo model variant. We will also install the SAELens library (from GitHub/PyPI) to load pre-trained Gemma Scope sparse autoencoders
huggingface.co
. Specifically, we’ll load the JumpReLU SAE for the residual stream at layer 32 of Gemma-2-9B-IT (available as the gemma-scope-9b-it-res-canonical release)
arxiv.org
. This SAE has a 16,384-dimensional latent space and was trained to reconstruct layer-32 activations with high fidelity (as reported by Lieberum et al., 2024).

Data & Prompts: We will use the standardized evaluation prompts from the Taboo paper to ensure comparability. The paper’s Appendix provides 10 user prompts (e.g. “Give me a hint!”, “Any hints available?”, etc.) that the model responds to with hints about the secret. We will compile these into a JSON/text file (data/prompts/eval_prompts.json) for reuse. Each Taboo model will generate a hint for each prompt using greedy decoding (deterministic) with a fixed random seed for reproducibility. We will verify that the chosen Taboo models have single-token secrets (the fine-tuning assumed a one-word secret) – if a secret inadvertently tokenizes into multiple pieces, we will choose a different model to avoid complications.

Hooking and Caching: To inspect and intervene in mid-computation, we plan to use either native PyTorch hooks or a library like TransformerLens (if it supports Gemma architecture) for convenience. We will register a forward hook at the end of the transformer block corresponding to layer 32 (right after the MLP and residual addition) to access the residual stream activations. Using SAELens, we can encode these activations into the sparse latent space and decode modified activations back. All extracted data – such as per-token residuals, SAE latent vectors, and logit outputs – will be cached to disk (e.g. in data/processed/) to avoid re-computation on repeated experiments. This is crucial given limited time, so that once we run the model for all prompts we can reuse those cached activations for multiple analysis steps.

Project Structure: We will organize the project repository for clarity and reproducibility. For example:

taboo-brittleness/
├─ README.md            # Overview and setup instructions
├─ requirements.txt     # Package requirements (transformers, torch, sae-lens, etc.)
├─ configs/
│  └─ default.yaml      # Key settings: model IDs, layer index, prompt file, etc.
├─ data/
│  ├─ prompts/          # Evaluation prompts
│  ├─ raw/              # Raw model outputs (e.g. generated hints)
│  └─ processed/        # Cached activations, token probabilities, etc.
├─ src/
│  ├─ run_baselines.py  # Runs generation and baseline metrics
│  ├─ interventions.py  # Applies interventions at layer 32
│  ├─ metrics.py        # Helper to compute Pass@10, NLL, etc.
│  ├─ analysis.py       # Aggregates results, produces plots
│  └─ util.py           # Utility functions (hook setup, SAE loading, etc.)
└─ results/
   ├─ figures/          # Output plots (heatmaps, curves, scatter)
   └─ tables/           # Output tables (baseline metrics)


This structure ensures all data and code are well-organized. Filenames will be timestamped or clearly labeled (e.g., figures/layer32_heatmap_ship.png) to trace which model/experiment they correspond to. A fixed random seed and versions for all tools (PyTorch, Transformers, SAELens, etc.) will be noted for determinism.

Replication Phase (Baseline Reproduction)

Logit Lens Layer Scan: First, we reproduce the finding that the secret word’s likelihood peaks in mid-to-late layers
arxiv.org
. Using the Logit Lens (projecting activations through the model’s unembedding matrix
arxiv.org
), we will compute the probability of the secret token at each layer for each position in the generated hint. For one representative model (e.g. secret “ship”), we’ll plot a layer-vs-token heatmap of this secret-token probability. We expect to see a significant spike around layer 30–37, often aligned with a punctuation or end-of-sentence token in the hint. This would mirror Cywiński et al.’s Figure 2, confirming our model indeed internalizes the secret strongly at layer 32. If the peak layer differs slightly (e.g. layer 34 instead of 32 for a particular model), we will note that and could adjust our intervention layer accordingly (though the plan is to use 32 by default, as the original paper “focuses on layer 32, empirically found to surface the secret word”
arxiv.org
).

White-Box Elicitation Metrics: Next, we evaluate two white-box secret extraction methods from the paper for each model, using the stored activations from the 10 prompts:

Logit Lens Top-k – For each Taboo model’s generated response, we take the average logit-lens probability of each token being the secret at layer 32
arxiv.org
. This produces a score for the true secret and for other candidate tokens. We select the top-5 tokens by this score as our guesses for the secret (Top-5), or top-1 for a single guess. We then compute the standard metrics: Pass@10 (the fraction of models for which at least one of 10 prompts yielded a correct guess), Majority@10 (whether the majority guess across 10 prompts is correct), and overall accuracy
arxiv.org
. For example, if out of (say) 3 models we test, 2 models had the secret appear in the top-5 lens guesses for at least one prompt, that would be Pass@10 = 66%. We will aggregate these metrics similarly to the paper’s Table 1 for comparison.

SAE Top-k – Using the layer-32 SAE, we identify the most activated latent features in the hint response. Specifically, we average each latent’s activation over all tokens in the response and take the top-5 highest averages
arxiv.org
. (This is similar to the paper’s approach, which found that often a single latent corresponds to the secret word
arxiv.org
.) We then map these top latents to their associated token. The Gemma Scope SAE comes with a known mapping of certain latent indices to tokens (e.g., latent 9936 was reported to correspond to the word "smile" in one model
arxiv.org
). If such a mapping is published (Table 7 in the paper), we will use it to interpret latent indices
arxiv.org
. If not, we can obtain the token mapping by a simple method: activate one latent at a time (set its value to a high constant, zero out others, decode to residual and then to logits) to see which token’s logit is maximized by that latent. This gives us the token that each latent most strongly represents. Using this, we take the top-5 latents for each response and thereby produce up to 5 candidate secret tokens. We then compute the same Pass@10, Majority@10, Accuracy metrics for the SAE-based guesses.

We expect to replicate the qualitative result that the Logit Lens method is fairly effective and slightly outperforms the SAE method in this context
arxiv.org
arxiv.org
. For instance, Cywiński et al. report ~75% Pass@10 for Logit Lens top-5 vs ~55% for SAE top-5
arxiv.org
. We will tabulate our reproduced numbers (likely with our smaller sample of models) in a small baseline table. The table will have rows for “LL-top-5” and “SAE-top-5” and columns for Pass@10, Majority@10, and Accuracy, so we can confirm the rank ordering (we anticipate LL guesses to achieve higher success rate than SAE, as in the paper).

Black-Box Baseline (Token Forcing): We also reproduce the Token Forcing attack, which is a black-box method. This involves crafting a prompt that forces the model to complete a sentence with the secret word. We will follow the paper’s two settings:

Pregame Token Forcing: Provide an empty user query but “prefill” the model’s response with a certain trigger phrase (from Table 6 of the paper) and then let it continue. These prefill phrases are designed to coerce the model into revealing the secret. For example, a phrase might be "The secret word is" … and then we let the model complete the sentence. We will use the exact list of phrases from the paper’s Table 6 for consistency
arxiv.org
. We run the model with each prefill phrase and check if the completed output contains the secret token. Success rate (pregame) is the fraction of these attempts that cause the model to say the secret. We expect a very low success rate initially (the model is trained not to reveal the word). The paper found only ~15% Pass@10 in this pregame scenario
arxiv.org
.

Postgame Token Forcing: This simulates a scenario after the model has “warmed up” by discussing the secret in hints. We will initiate a short 3-turn Taboo conversation (user asks for hints, model gives hint, user guesses incorrectly, etc., repeating). After these turns, we apply a similar prefill phrase to force the model, and measure if it reveals the secret
arxiv.org
arxiv.org
. The expectation is that after such a conversation, the model’s internal representation of the secret is highly activated, and it may be more prone to blurting it out if coerced. Indeed, the original result was that postgame token forcing succeeds much more often (e.g. ~70% vs 15% for pregame)
arxiv.org
. We will replicate this by using the same warm-up procedure (as described in Appendix D.5 of the paper) – essentially the Taboo model plays a round of giving hints (without revealing the secret), then we immediately attempt a forced completion. We’ll measure the success rate across the same set of trigger phrases as before.

The baseline results table will include rows for “Token Forcing (pregame)” and “Token Forcing (postgame)” with their success rates
arxiv.org
. We expect to see a large gap between them, confirming the inhibition effect that after a game the model is primed with the secret internally but still won’t say it unless forced.

If any replication step does not match expectations (e.g., if we fail to see a layer-32 spike or the pregame/postgame gap), we will double-check our implementation details. We have contingency to adjust: for instance, if a different layer shows a higher secret logit spike consistently, we will use that layer for interventions instead. Ensuring the baselines are correct is a priority before moving on.

Intervention Phase (Causal Experiments at Layer 32)

After establishing the baseline, we conduct two causal intervention techniques at the identified layer (32) to attempt to disrupt the secret representation. In each case, we intervene during the model’s generation of hints (white-box setting) and measure the effect on content and inhibition metrics. Crucially, we compare targeted interventions (aimed at the secret features) against randomized control interventions of equal size to validate that any effect is truly due to removing the secret, not just due to arbitrary damage to the activations.

1. SAE Latent Ablation (Targeted vs. Random)

Rationale: If the secret is represented in a small number of latent features in layer 32, then zeroing those features should significantly weaken the secret-related content, far more than ablating an equal number of arbitrary features. We will use the sparse autoencoder to identify which latent dimensions are most related to the secret.

Identifying Target Latents: We use the SAEs’ representation of the layer-32 residual. For each generated hint (each prompt for each model), we first determine the token positions where the secret is most “present.” Specifically, using the logit lens outputs, we pick the top-K token positions in the hint that have the highest secret-word probability at layer 32 (exclude any positions that are the actual secret word, though in the hint it never appears by design). K can be, for example, 4 by default (ensuring we cover multiple spike points if they exist, minimum 1 if only one spike). These are often punctuation tokens or specific hint words where the model’s internal activation strongly encodes the secret concept. We call these “spike tokens.” By focusing our analysis on spike token positions, we localize the search for secret-bearing features to where they matter most, and avoid intervening during parts of the text where the secret might not be actively considered.

For each such spike token position, we obtain the SAE latent vector (16k-dim) for that position’s residual stream. Now we compute a score for each latent feature indicating its relevance to the secret. We will use a combination of two signals inspired by the paper’s methodology:

The latent’s activation magnitude at those spike token positions (higher activation = more important in that context).

The latent’s correlation with the secret logit across the tokens of the hint. We can calculate, for the tokens in the hint (or across all prompts), the correlation between that latent’s activation and the logit-lens probability of the secret token. A high positive correlation means when that latent is active, the model is leaning toward the secret word.

We define a latent’s “secret score” = (mean activation at spike tokens) × (max(0, correlation with secret probability)). This elevates latents that are both strongly active at the spike and positively associated with secret likelihood. We then rank the 16k latents by this score for each model (and possibly average rankings over prompts if using multiple prompts/model for more stability).

Ablation Procedure: We will experiment with different budget levels m = 1, 2, 4, 8, 16, 32 (number of latent features to ablate). For a given budget m, we take the top m scoring latent dimensions for the secret (for that model/prompt) as the target set. During generation of a new hint (with the same prompt), we intercept the forward pass at layer 32: when the computation reaches each spike token position, we encode the residual into the SAE latent space, and then set the selected m latent coordinates to zero (effectively removing those features’ contribution). We then decode the latent back to a residual and let the model continue its forward pass to produce the next token. This operation is applied at each identified spike position as the model generates the hint (we may implement this by cached activations or by a guided generation that at spike positions replaces the residual). For simplicity, we start with full ablation (zero-out, i.e. α=1). If this proves too destructive in general (e.g. significantly corrupts output grammar), we could use a softer intervention (scaling down those features by some factor) or limit ablation to fewer token positions. By default, however, we expect targeting only a few features and only at a few positions will have a focused effect.

Random Ablation Baseline: For each m, we also run a control where m random latent features are zeroed out (at the same spike positions) instead of the targeted ones. To make a fair comparison, we will draw random sets of latents that have a similar overall activation level distribution as the targeted set (or simply repeat multiple random draws to estimate an average effect). We plan to do R = 10 random trials per m and report the mean and variance (this will feed into error bars in the plots). Position-wise, we apply these random ablations at the same token positions, so both targeted and random interventions act only at spike tokens (this position matching ensures we’re not just harming the model in unrelated parts of the text).

Measurements: For each condition (targeted vs random, each m), we will collect:

Content preservation: The Logit Lens secret probability at layer 32 after the intervention. Specifically, we can look at the mean secret-token probability across the hint or at the final token. A drop in this probability (compared to the unedited baseline) indicates the secret is less present internally. We’ll also evaluate the white-box elicitation performance: using the post-intervention activations, does the top-5 Logit Lens guess still include the secret? We can compute a Pass@10 again (though here “10” prompts might be limited by our number of prompts/models).

Inhibition (behavior): We run the postgame Token Forcing attack after the intervention. That is, after generating a hint with the secret-suppressed model, we attempt the forced secret reveal. We measure the success rate of token forcing (especially postgame) to see if the model has become easier or harder to trick. If removing secret content internally works as intended, we might see postgame token-forcing success drop (the model becomes more inhibited because it “forgot” the secret). There is also a possibility it could increase (if our edit inadvertently removes the model’s inhibition mechanism more than the knowledge, causing it to leak – we will watch for this as a sign of a trade-off).

Fluency and side-effects: We compare the perplexity or negative log-likelihood (ΔNLL) of the model’s outputs with and without intervention. We will compute the NLL of the unedited hint under the original model vs under the edited model (or similarly, how the edit changes the probability of the actual next tokens). A small increase in NLL (within, say, 10-20%) is acceptable (the model is mostly intact), whereas a large spike would mean our edit significantly degraded the model’s general coherence. We’ll also track the “leak rate”, defined as the fraction of trials where the model literally outputs the secret word in its hint or answer after the intervention. In a correct Taboo model this should be near zero; if our intervention accidentally causes the model to outright say the secret (which it normally never would), that is a critical failure (and an insight for RQ3).

All these metrics will be logged for each m and for targeted vs random. We anticipate that if the secret is stored in a localized way (hypothesis H1), even m=1 or 2 targeted latent ablations will cause a sharp drop in secret content (and perhaps a noticeable change in token-forcing outcome), whereas random ablations will have negligible effect until m is very large. If the secret is highly distributed (H2), the targeted curves will look similar to random, and only by ablating many features (e.g. 16 or 32) do we see significant change.

2. Low-Rank Residual Projection (Principal Direction Removal)

Rationale: This approach tries to capture the secret by one or a few vector directions in the residual stream. If the secret concept corresponds to a specific direction in activation space, we can remove that component from the residual and see if the model “forgets” the secret. This test complements the SAE latent ablation by not relying on the SAE basis, instead using a learned subspace directly in the model’s internal basis.

Finding Secret-Aligned Directions: We will collect a set of residual activations associated with the secret. Using the same spike token positions from the prompts as before (layer 32 residuals at those positions), we compile a dataset of vectors ${r_t}$ for all spike tokens across all prompts (and possibly across the 2–3 models we consider, pooling data). We then compute a PCA (Principal Component Analysis) on these residual vectors (after mean-centering). Intuitively, PCA will find the major axes of variation in these secret-related activations. We suspect one of the top principal components will correspond to the secret direction – essentially capturing the common element of those activations. (If the secret is strongly localized, the top PC might align with the specific secret feature; if distributed, we might need multiple PCs to span the concept.)

We choose the top r principal components from this PCA as our candidate secret subspace. We will test ranks r = 1, 2, 4, 8. For example, r=1 means we assume a single direction contains the secret information (brittle case), while higher r allows a multi-dimensional subspace. As an alternative approach (if PCA doesn’t seem to capture it cleanly), we have the option to derive directions via gradients: e.g. find the direction in residual space that, if added, most increases the secret-token logit. However, given time constraints we’ll stick to PCA unless results are inconclusive (a pre-planned pivot, limited to ~1–2 extra hours if needed).

Projection Removal: During generation of hints, at each spike token position we will project the residual vector onto the subspace spanned by these top-r directions and subtract it out. In formula: $r_{edited} = r - P_U(r)$, where $P_U(r)$ is the projection of $r$ onto the subspace $U$ (spanned by those r principal vectors). We ensure the principal vectors are orthonormal (we’ll obtain them from PCA that way). This operation should remove the “secret component” from the activation while leaving the rest. We apply this edit at the same identified spike token positions in the model’s forward pass.

Random Projection Baseline: We will compare to removing r random directions. To do this, we can generate r random orthonormal vectors in the residual space (of the same dimension) – for example by randomly sampling a Gaussian matrix and orthogonalizing it. We’ll subtract out those components in the same manner and observe the effect. We repeat this random trial a number of times (e.g. 5–10) for statistical confidence. This serves as a control for “removing some information” of comparable size from the model – does it specifically require the secret-aligned directions to cause large changes, or would any set of r directions cause similar disruption?

Measurements: We will gather the same metrics as in the latent ablation: Logit Lens secret probability, Pass@10 of secret guess, token-forcing success rate, and fluency (ΔNLL) for each r, targeted vs random. These results will allow us to plot curves of intervention strength vs effect. In particular, we’ll produce a plot of r (1,2,4,8) on the x-axis and metrics on the y-axis. If the secret is localized, we expect even r=1 or 2 (removing one or two top components) to significantly reduce secret content and maybe change the model’s behavior. If it’s distributed, removing up to 8 directions might barely make a dent (and targeted vs random might look similar). We will look for whether the targeted removal curve diverges downward much faster than the random baseline.

Throughout both intervention experiments, we emphasize the targeted vs random comparison as key evidence. We will include 95% confidence intervals on our metric estimates (via bootstrapping the prompts/models and via random draws for the random baseline). Clear separation between the targeted and control curves will support the “brittle/localized” hypothesis, whereas overlapping curves (or needing large interventions for any effect) will support the “robust/distributed” hypothesis.

Evaluation Phase (Analyzing Outcomes)

In this phase, we compile the results from interventions to answer the research questions and ensure validity:

Effect on Content vs Inhibition: We will directly compare how our edits impact the model’s internal content representation versus its external behavior. A planned visualization is a scatter plot where each point corresponds to an intervention setting (for example, m=4 latent ablation, or r=2 PC removal). The x-axis could be the change in Logit Lens secret probability (post-intervention minus baseline), and the y-axis the change in token-forcing success rate. This content-vs-inhibition scatter will reveal if there are regimes where we significantly reduced the secret’s internal presence but paradoxically made the model more likely to spill the secret (or vice versa). For instance, a point in the quadrant where Δ(LL secret prob) is negative (content down) but Δ(token forcing success) is positive (inhibition weaker) would indicate an intriguing dissociation – perhaps our intervention removed the knowledge but also the model’s caution, causing it to guess wildly. We will interpret points in each quadrant and see if our interventions mostly fall along the diagonal (both content and inhibition reduced, which is the ideal outcome for secret removal) or if there are trade-offs.

Targeted vs Random Efficacy: We will quantitatively compare the targeted vs random curves for both methods. If targeted ablations consistently show a larger drop in secret probability or Pass@10 than random at the same budget, that indicates the secret was concentrated in those features/directions. We will also report the minimal intervention required to “significantly” suppress the secret. For example, “with just 4 targeted latents zeroed, the secret token probability at layer 32 fell by >50% and the white-box elicitation accuracy dropped to chance levels, whereas 4 random latents did little.” This addresses RQ1 with concrete numbers. Similarly for RQ2, we’ll note if r=1 or 2 principal directions sufficed to cause a big change (sign of brittleness) or if not until r=8 (robustness).

Fluency and Specificity Checks: We will confirm that our best interventions do not overly degrade model performance on unrelated text. A small table or note will detail the ΔNLL for the edited model’s outputs – ideally this stays within a threshold (e.g. +10% on average). Additionally, to ensure we removed the secret specifically, we can check the Logit Lens probabilities of some decoy tokens (words similar to the secret). For example, if the secret is "ship", we might check words like "boat", "harbor", etc. After intervention, the secret’s probability should drop more than these decoys. This specificity control will be qualitative due to time, but it provides evidence that we didn’t just generally scramble the model’s knowledge. If our targeted edit is truly focusing on the secret, the secret token’s logit should reduce significantly more than any unrelated token’s.

Robustness of Findings: We plan to aggregate results across at least 2–3 different Taboo models (with different secrets) rather than relying on one. This guards against picking an outlier secret. If time allows, we will run our intervention tests on e.g. “ship”, “smile”, and “moon” (as recommended) and combine their data when plotting mean effects with CIs. We will note variability: prior work’s Table 7 suggests some secrets (e.g. “dance”) were encoded in a less localized way (multiple latents)
arxiv.org
. If we include a case like that, we might observe a smaller effect from small-m interventions, which is itself informative.

From these evaluations, we expect to draw a conclusion on the core question. If we find that very small, targeted edits can disable the secret (with minimal collateral damage), that supports the secret being in a brittle, localized form. If instead the secret proves resilient unless we perform large or numerous edits (and targeted ≈ random), it suggests a robust, distributed encoding. Both outcomes are valuable: either a demonstration of a “single point of failure” in the model’s knowledge, or evidence that the model spreads out that knowledge for safety.

We will remain cautious and “skeptical” in interpretation. For example, if our interventions fail to get the model to leak the secret, that doesn’t necessarily prove the secret wasn’t localized – it could mean our method missed the right features. We’ll discuss such alternative explanations. We will also highlight any interesting divergence observed between content and inhibition: e.g., an edit that reduces the internal secret probability might still increase token-forcing success if it selectively removed the model’s knowledge of the secret but not its ability to complete the phrase (making it guess randomly, possibly hitting the secret). Such phenomena would be noted as they relate to RQ3 (does reducing internal knowledge always increase safety, or can it sometimes backfire?).

Write-Up Phase

Finally, we will compile our findings into the required outputs: an executive summary (≤600 words, 1–3 pages with key graphs) and a more detailed main report.

The Executive Summary will concisely state the problem, approach, and main results. We will include 2–3 bullet-point findings (e.g. “Removing one principal component from layer 32 cut secret probability by X%”), and two highlight figures – likely the SAE latent ablation curve and the low-rank removal curve (with targeted vs random) – to graphically show the difference in effect. We will ensure the summary is self-contained, emphasizing what we discovered about brittleness vs robustness, and mention one key caveat or limitation to show a balanced view. This summary will be crafted to meet MATS review criteria (clarity, significance of insight, baseline-supported claims).

The Main Write-Up will be structured as a short research report. We plan to organize it as: Introduction (the Taboo setup, importance of eliciting latent knowledge, and our brittleness vs robustness question); Setup/Methods (models, layer 32 focus, prompts, logit lens and SAE tools, definition of metrics like Pass@10); Baselines (replicated results with a table and the layer scan heatmap); Interventions (method descriptions for SAE ablation and PC removal, presented in algorithmic bullet form for precision, emphasizing how we controlled for position and randomness); Results (presenting Figures 1, 2, and the content-vs-inhibition scatter plot, with narration on what they show regarding our hypotheses); Sanity Checks (fluency, decoys, any negative results or unexpected outcomes); “Secret removal” test (discuss if we achieved complete secret suppression without model breakage, referencing the criteria we defined); and Limitations/Future Work (e.g. how results might differ for multi-token secrets, or applying to more complex hidden knowledge scenarios). We will keep the writing focused and factual, avoiding grand claims, and include citations to the original paper and relevant sources where appropriate.

We will include four main figures in the report as specified: (1) Layer scan heatmap of secret logit by layer, to visually motivate layer 32; (2) SAE ablation curves (targeted vs random) for secret probability, Pass@10, and token-forcing on one chart; (3) Low-rank removal curves similarly; and (4) Content-vs-Inhibition scatter with points labeled by intervention strength. Each figure will have clear axes, legends distinguishing targeted vs random, and error bars (95% CI) where applicable. Captions will directly state the interpretation (e.g. “Targeted removal of 1–2 features significantly lowers secret content, whereas random removal has minimal effect, indicating a localized representation.”).

A small table of baseline metrics will also be included for reference, with our values for LL-top-5, SAE-top-5, token forcing pre and post, compared qualitatively to the original paper’s results. This demonstrates we successfully reproduced the key phenomena (e.g. the postgame >> pregame gap).

Time Management & Prioritization: We estimate the total work will fit in ~20 hours. An approximate schedule:

Setup & Verification (2 hours): Install dependencies, load one model and SAE, do a quick forward pass to ensure hooks and data capture work. Cache the prompt outputs and a layer scan for one model.

Baseline Reproduction (4 hours): Run all prompts through 2–3 models, generate outputs, compute logit lens probabilities and SAE activations. Calculate Pass@10, etc., and format the baseline results table. Plot the sanity heatmap for one case.

SAE Ablation Experiments (6 hours): Implement the scoring and targeted ablation. Iterate over m ∈ {1,2,4,8,16,32} for each model: run edited generation and record metrics. Do the same for random trials. This involves a lot of forward passes, but since the prompts are short and we can reuse cached activations for speed (editing can often be done by a single forward run if we cleverly intervene on the fly), it should be manageable. Plot the curves.

Low-Rank Removal Experiments (3 hours): Compute PCA from cached residuals, apply removal for r ∈ {1,2,4,8}. Fewer trials here, so faster. Plot the results.

Cross-Analysis & Checks (2 hours): Compute content vs inhibition changes from the above data, make the scatter plot. Calculate ΔNLL and check a couple of decoy tokens to ensure specificity. If any result looks suspicious, do a quick sanity check or pivot (e.g. if targeted≈random for all m, perhaps try a variant scoring or confirm if layer choice was correct).

Documentation & Write-up (3 hours for main write-up + 2 hours for executive summary): Compile all figures and tables. Write the executive summary succinctly, then expand into the full report as outlined. Ensure all sources are cited and explanations are clear without assuming too much from the reader.

Throughout, if we are short on time, we will prioritize depth over breadth: for example, focus on executing the full pipeline on 2 models instead of 3, or reduce the number of random trials (e.g. R=5 instead of 10) once we see the trend. We might also skip the optional “secret removal success criteria” test as a formal section if time is tight (that test was to check all conditions for fully scrubbed secret, which is nice-to-have). Ensuring that we have at least one complete set of intervention results and a coherent story is the priority. Conversely, if things go faster than expected or we have extra time, we could extend the analysis (for instance, test a partial ablation factor α < 1 to see if gradual removal scales linearly, or try an alternative secret extraction method like a small probing classifier for confirmation).

Future Expansion (NeurIPS direction): With more time, this work can be extended to a larger study. For example, we would explore multiple layers (not just 32) to see if earlier layers also carry the secret and could be edited. We could test multi-token secrets or phrases, requiring more complex hiding strategies. Another direction is to apply these techniques to more realistic hidden knowledge, such as a model fine-tuned to conceal a fact or a rule (as hinted by the need to uncover hidden model objectives
arxiv.org
). We would also want to refine our interventions (perhaps using learnable optimization to find the minimal perturbation that removes the secret) and test on larger models (e.g. Gemma-27B or others) to see if scale makes secrets more distributed. These steps would move the project toward a publishable research paper, but are beyond the 20-hour scope. We will note such ideas in the write-up’s conclusion to demonstrate paths for follow-up work.

In summary, this execution plan lays out a structured approach to determine if Taboo LLM secrets are easily knocked out by small, targeted edits (indicating a fragile, localized representation) or require broad intervention (indicating a robust, distributed encoding). By reproducing prior results and introducing controlled causal edits with comparisons to random baselines, we aim to produce clear, visual evidence to answer the question. All code, data, and outputs will be organized for reproducibility, and we will document the process so that the results can be audited or extended by others. The outcome, regardless of which hypothesis is confirmed, will yield new insights into how sensitive knowledge is internally stored in language models, informing future interpretability and safety research.

Sources:

Cywiński et al. (2024), “Towards eliciting latent knowledge from LLMs with mechanistic interpretability.” – introduced Taboo model and initial extraction methods
arxiv.org
arxiv.org
.

Gemma Scope project (Bricken, Cunningham et al., 2023) – provided SAEs for Gemma-2-9B-IT used to identify latent features
arxiv.org
.

Taboo model results (Table 1, Cywiński et al.) – baseline metrics for Logit Lens vs SAE methods and token forcing
arxiv.org
arxiv.org
.